{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Download necessary NLTK resources\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:26:44.258451Z","iopub.execute_input":"2025-04-27T13:26:44.259024Z","iopub.status.idle":"2025-04-27T13:26:44.265429Z","shell.execute_reply.started":"2025-04-27T13:26:44.259002Z","shell.execute_reply":"2025-04-27T13:26:44.264733Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:27:09.473633Z","iopub.execute_input":"2025-04-27T13:27:09.474215Z","iopub.status.idle":"2025-04-27T13:27:10.117868Z","shell.execute_reply.started":"2025-04-27T13:27:09.474191Z","shell.execute_reply":"2025-04-27T13:27:10.117250Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:27:16.915952Z","iopub.execute_input":"2025-04-27T13:27:16.916453Z","iopub.status.idle":"2025-04-27T13:27:16.986733Z","shell.execute_reply.started":"2025-04-27T13:27:16.916431Z","shell.execute_reply":"2025-04-27T13:27:16.986156Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"                                                   review sentiment\ncount                                               50000     50000\nunique                                              49582         2\ntop     Loved today's show!!! It was a variety and not...  positive\nfreq                                                    5     25000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49582</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Loved today's show!!! It was a variety and not...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:27:28.194342Z","iopub.execute_input":"2025-04-27T13:27:28.194584Z","iopub.status.idle":"2025-04-27T13:27:28.208679Z","shell.execute_reply.started":"2025-04-27T13:27:28.194567Z","shell.execute_reply":"2025-04-27T13:27:28.207964Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"review       0\nsentiment    0\ndtype: int64"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"df.drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:34:39.838579Z","iopub.execute_input":"2025-04-27T13:34:39.839134Z","iopub.status.idle":"2025-04-27T13:34:40.013348Z","shell.execute_reply.started":"2025-04-27T13:34:39.839112Z","shell.execute_reply":"2025-04-27T13:34:40.012648Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      One of the other reviewers has mentioned that ...  positive\n1      A wonderful little production. <br /><br />The...  positive\n2      I thought this was a wonderful way to spend ti...  positive\n3      Basically there's a family where a little boy ...  negative\n4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n...                                                  ...       ...\n49995  I thought this movie did a down right good job...  positive\n49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n49997  I am a Catholic taught in parochial elementary...  negative\n49998  I'm going to have to disagree with the previou...  negative\n49999  No one expects the Star Trek movies to be high...  negative\n\n[49582 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>I thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>I am a Catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>I'm going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>No one expects the Star Trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>49582 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"# Preprocessing\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    text = BeautifulSoup(text, \"html.parser\").get_text()\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+|#', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words]\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ndf['processed_text'] = df['review'].apply(preprocess_text)\nprint(df[['review', 'processed_text']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:36:33.922781Z","iopub.execute_input":"2025-04-27T13:36:33.923357Z","iopub.status.idle":"2025-04-27T13:37:39.312084Z","shell.execute_reply.started":"2025-04-27T13:36:33.923332Z","shell.execute_reply":"2025-04-27T13:37:39.311291Z"}},"outputs":[{"name":"stdout","text":"                                              review  \\\n0  One of the other reviewers has mentioned that ...   \n1  A wonderful little production. <br /><br />The...   \n2  I thought this was a wonderful way to spend ti...   \n3  Basically there's a family where a little boy ...   \n4  Petter Mattei's \"Love in the Time of Money\" is...   \n\n                                      processed_text  \n0  one reviewer mentioned watching 1 oz episode y...  \n1  wonderful little production filming technique ...  \n2  thought wonderful way spend time hot summer we...  \n3  basically there family little boy jake think t...  \n4  petter matteis love time money visually stunni...  \n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Encode labels\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:38:25.294263Z","iopub.execute_input":"2025-04-27T13:38:25.294529Z","iopub.status.idle":"2025-04-27T13:38:25.311467Z","shell.execute_reply.started":"2025-04-27T13:38:25.294508Z","shell.execute_reply":"2025-04-27T13:38:25.310511Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  \\\n0  One of the other reviewers has mentioned that ...  positive   \n1  A wonderful little production. <br /><br />The...  positive   \n2  I thought this was a wonderful way to spend ti...  positive   \n3  Basically there's a family where a little boy ...  negative   \n4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n\n                                      processed_text  sentiment_encoded  \n0  one reviewer mentioned watching 1 oz episode y...                  1  \n1  wonderful little production filming technique ...                  1  \n2  thought wonderful way spend time hot summer we...                  1  \n3  basically there family little boy jake think t...                  0  \n4  petter matteis love time money visually stunni...                  1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>processed_text</th>\n      <th>sentiment_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>one reviewer mentioned watching 1 oz episode y...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n      <td>wonderful little production filming technique ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n      <td>basically there family little boy jake think t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n      <td>petter matteis love time money visually stunni...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Split the data\nX_train, X_temp, y_train, y_temp = train_test_split(\n    df['processed_text'], df['sentiment_encoded'], test_size=0.3, random_state=42, stratify=df['sentiment']\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n)\nprint(f\"\\nTraining set size: {len(X_train)}\")\nprint(f\"Validation set size: {len(X_val)}\")\nprint(f\"Test set size: {len(X_test)}\")\n\n# Save splits\ntrain_df = pd.DataFrame({'processed_text': X_train, 'sentiment_encoded': y_train})\nval_df = pd.DataFrame({'processed_text': X_val, 'sentiment_encoded': y_val})\ntest_df = pd.DataFrame({'processed_text': X_test, 'sentiment_encoded': y_test})\n\ntrain_df.to_csv('train_data.csv', index=False)\nval_df.to_csv('val_data.csv', index=False)\ntest_df.to_csv('test_data.csv', index=False)\n\nprint(\"\\nData splits saved as 'train_data.csv', 'val_data.csv', and 'test_data.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:49:22.469803Z","iopub.execute_input":"2025-04-27T13:49:22.470341Z","iopub.status.idle":"2025-04-27T13:49:23.730106Z","shell.execute_reply.started":"2025-04-27T13:49:22.470318Z","shell.execute_reply":"2025-04-27T13:49:23.729211Z"}},"outputs":[{"name":"stdout","text":"\nTraining set size: 35000\nValidation set size: 7500\nTest set size: 7500\n\nData splits saved as 'train_data.csv', 'val_data.csv', and 'test_data.csv'\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load tokenizer and model\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2,\n    output_attentions=False,\n    output_hidden_states=False,\n)\nmodel.to(device)\n\n# Load data splits\ntrain_df = pd.read_csv('train_data.csv')\nval_df = pd.read_csv('val_data.csv')\ntest_df = pd.read_csv('test_data.csv')\n\n# Tokenize data\ndef tokenize_data(texts, labels, tokenizer, max_length=128):\n    input_ids = []\n    attention_masks = []\n    \n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels.values, dtype=torch.long)\n    \n    return input_ids, attention_masks, labels\n\n# Tokenize datasets\ntrain_inputs, train_masks, train_labels = tokenize_data(train_df['processed_text'], train_df['sentiment_encoded'], tokenizer)\nval_inputs, val_masks, val_labels = tokenize_data(val_df['processed_text'], val_df['sentiment_encoded'], tokenizer)\ntest_inputs, test_masks, test_labels = tokenize_data(test_df['processed_text'], test_df['sentiment_encoded'], tokenizer)\n\n# Create DataLoaders\nbatch_size = 16  # Default, will be tuned in training\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n# Save model and tokenizer\nmodel.save_pretrained('sentiment_model')\ntokenizer.save_pretrained('sentiment_model')\n\nprint(\"Model and tokenizer saved in 'sentiment_model' directory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T13:50:08.856917Z","iopub.execute_input":"2025-04-27T13:50:08.857480Z","iopub.status.idle":"2025-04-27T13:52:05.167194Z","shell.execute_reply.started":"2025-04-27T13:50:08.857456Z","shell.execute_reply":"2025-04-27T13:52:05.166576Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer saved in 'sentiment_model' directory\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('sentiment_model')\n\n# Load data splits\ntrain_df = pd.read_csv('train_data.csv')\nval_df = pd.read_csv('val_data.csv')\n\n# Tokenize data\ndef tokenize_data(texts, labels, tokenizer, max_length=128):\n    input_ids = []\n    attention_masks = []\n    \n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels.values, dtype=torch.long)\n    \n    return input_ids, attention_masks, labels\n\ntrain_inputs, train_masks, train_labels = tokenize_data(train_df['processed_text'], train_df['sentiment_encoded'], tokenizer)\nval_inputs, val_masks, val_labels = tokenize_data(val_df['processed_text'], val_df['sentiment_encoded'], tokenizer)\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\n\n# Hyperparameter grid\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [16, 32]\nbest_val_accuracy = 0\nbest_hyperparams = {}\nbest_model_path = 'best_model.pt'\n\n# Training function\ndef train_model(model, train_dataloader, val_dataloader, lr, epochs=5):\n    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n    \n    early_stopping_patience = 2\n    best_val_loss = float('inf')\n    early_stopping_counter = 0\n    \n    train_loss_values = []\n    val_loss_values = []\n    train_accuracy_values = []\n    val_accuracy_values = []\n    \n    for epoch in range(epochs):\n        print(f'\\nEpoch {epoch + 1}/{epochs}')\n        model.train()\n        total_train_loss = 0\n        train_predictions, train_true_labels = [], []\n        \n        for step, batch in enumerate(train_dataloader):\n            b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]\n            model.zero_grad()\n            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            total_train_loss += loss.item()\n            logits = outputs.logits\n            train_predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n            train_true_labels.extend(b_labels.cpu().numpy())\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            if step % 100 == 0 and step != 0:\n                print(f\"Step {step}/{len(train_dataloader)} | Loss: {loss.item():.4f}\")\n        \n        avg_train_loss = total_train_loss / len(train_dataloader)\n        train_loss_values.append(avg_train_loss)\n        avg_train_accuracy = sum(pred == true for pred, true in zip(train_predictions, train_true_labels)) / len(train_true_labels)\n        train_accuracy_values.append(avg_train_accuracy)\n        \n        # Validation\n        model.eval()\n        total_val_loss = 0\n        predictions, true_labels = [], []\n        \n        for batch in val_dataloader:\n            b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]\n            with torch.no_grad():\n                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            logits = outputs.logits\n            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n            true_labels.extend(b_labels.cpu().numpy())\n        \n        avg_val_loss = total_val_loss / len(val_dataloader)\n        val_loss_values.append(avg_val_loss)\n        val_accuracy = sum(pred == true for pred, true in zip(predictions, true_labels)) / len(true_labels)\n        val_accuracy_values.append(val_accuracy)\n        \n        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n        print(f\"Average Training Accuracy: {avg_train_accuracy:.4f}\")\n        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n        \n        # Early stopping\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= early_stopping_patience:\n                print(\"Early stopping triggered.\")\n                break\n    \n    return val_accuracy, train_loss_values, val_loss_values, train_accuracy_values, val_accuracy_values\n\n# Hyperparameter tuning loop\nfor lr in learning_rates:\n    for batch_size in batch_sizes:\n        print(f\"\\nTraining with lr={lr}, batch_size={batch_size}\")\n        \n        # Create DataLoaders\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n        val_sampler = SequentialSampler(val_data)\n        val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n        \n        # Load fresh model\n        model = DistilBertForSequenceClassification.from_pretrained('sentiment_model')\n        model.to(device)\n        \n        # Train\n        val_accuracy, train_loss_values, val_loss_values, train_accuracy_values, val_accuracy_values = train_model(\n            model, train_dataloader, val_dataloader, lr\n        )\n        \n        # Save best model\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            best_hyperparams = {'lr': lr, 'batch_size': batch_size}\n            torch.save(model.state_dict(), best_model_path)\n            \n            # Visualize training progress\n            plt.figure(figsize=(12, 5))\n            plt.subplot(1, 2, 1)\n            plt.plot(train_loss_values, label='Training Loss')\n            plt.plot(val_loss_values, label='Validation Loss')\n            plt.title('Training and Validation Loss')\n            plt.xlabel('Epoch')\n            plt.ylabel('Loss')\n            plt.legend()\n            \n            plt.subplot(1, 2, 2)\n            plt.plot(train_accuracy_values, label='Training Accuracy')\n            plt.plot(val_accuracy_values, label='Validation Accuracy')\n            plt.title('Training and Validation Accuracy')\n            plt.xlabel('Epoch')\n            plt.ylabel('Accuracy')\n            plt.legend()\n            plt.savefig(f'loss_accuracy_lr{lr}_bs{batch_size}.png')\n            plt.close()\n\nprint(f\"\\nBest Hyperparameters: {best_hyperparams}\")\nprint(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\nprint(\"Training complete. Best model saved as 'best_model.pt'. Loss and accuracy curves saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T14:22:18.897543Z","iopub.execute_input":"2025-04-27T14:22:18.898078Z","iopub.status.idle":"2025-04-27T16:37:31.627723Z","shell.execute_reply.started":"2025-04-27T14:22:18.898054Z","shell.execute_reply":"2025-04-27T16:37:31.627097Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTraining with lr=2e-05, batch_size=16\n\nEpoch 1/5\nStep 100/2188 | Loss: 0.4279\nStep 200/2188 | Loss: 0.4273\nStep 300/2188 | Loss: 0.3307\nStep 400/2188 | Loss: 0.3803\nStep 500/2188 | Loss: 0.1097\nStep 600/2188 | Loss: 0.3935\nStep 700/2188 | Loss: 0.0867\nStep 800/2188 | Loss: 0.2272\nStep 900/2188 | Loss: 0.1457\nStep 1000/2188 | Loss: 0.4021\nStep 1100/2188 | Loss: 0.6475\nStep 1200/2188 | Loss: 0.3837\nStep 1300/2188 | Loss: 0.2242\nStep 1400/2188 | Loss: 0.3552\nStep 1500/2188 | Loss: 0.0415\nStep 1600/2188 | Loss: 0.3451\nStep 1700/2188 | Loss: 0.1750\nStep 1800/2188 | Loss: 0.1120\nStep 1900/2188 | Loss: 0.4471\nStep 2000/2188 | Loss: 0.2078\nStep 2100/2188 | Loss: 0.3012\nAverage Training Loss: 0.3504\nAverage Training Accuracy: 0.8463\nAverage Validation Loss: 0.2754\nValidation Accuracy: 0.8839\n\nEpoch 2/5\nStep 100/2188 | Loss: 0.0130\nStep 200/2188 | Loss: 0.1277\nStep 300/2188 | Loss: 0.4172\nStep 400/2188 | Loss: 0.0777\nStep 500/2188 | Loss: 0.4621\nStep 600/2188 | Loss: 0.3629\nStep 700/2188 | Loss: 0.3844\nStep 800/2188 | Loss: 0.3154\nStep 900/2188 | Loss: 0.1926\nStep 1000/2188 | Loss: 0.4130\nStep 1100/2188 | Loss: 0.3572\nStep 1200/2188 | Loss: 0.0151\nStep 1300/2188 | Loss: 0.1787\nStep 1400/2188 | Loss: 0.1730\nStep 1500/2188 | Loss: 0.0533\nStep 1600/2188 | Loss: 0.5086\nStep 1700/2188 | Loss: 0.0138\nStep 1800/2188 | Loss: 0.3114\nStep 1900/2188 | Loss: 0.0608\nStep 2000/2188 | Loss: 0.0851\nStep 2100/2188 | Loss: 0.0129\nAverage Training Loss: 0.2247\nAverage Training Accuracy: 0.9159\nAverage Validation Loss: 0.3326\nValidation Accuracy: 0.8752\n\nEpoch 3/5\nStep 100/2188 | Loss: 0.4597\nStep 200/2188 | Loss: 0.0072\nStep 300/2188 | Loss: 0.0058\nStep 400/2188 | Loss: 0.0097\nStep 500/2188 | Loss: 0.0124\nStep 600/2188 | Loss: 0.0056\nStep 700/2188 | Loss: 0.0368\nStep 800/2188 | Loss: 0.1856\nStep 900/2188 | Loss: 0.0046\nStep 1000/2188 | Loss: 0.0999\nStep 1100/2188 | Loss: 0.0027\nStep 1200/2188 | Loss: 0.0164\nStep 1300/2188 | Loss: 0.1161\nStep 1400/2188 | Loss: 0.0136\nStep 1500/2188 | Loss: 0.0056\nStep 1600/2188 | Loss: 0.1870\nStep 1700/2188 | Loss: 0.1202\nStep 1800/2188 | Loss: 0.0111\nStep 1900/2188 | Loss: 0.4476\nStep 2000/2188 | Loss: 0.5982\nStep 2100/2188 | Loss: 0.5740\nAverage Training Loss: 0.1458\nAverage Training Accuracy: 0.9560\nAverage Validation Loss: 0.3748\nValidation Accuracy: 0.8941\nEarly stopping triggered.\n\nTraining with lr=2e-05, batch_size=32\n\nEpoch 1/5\nStep 100/1094 | Loss: 0.4414\nStep 200/1094 | Loss: 0.6734\nStep 300/1094 | Loss: 0.3817\nStep 400/1094 | Loss: 0.2770\nStep 500/1094 | Loss: 0.2062\nStep 600/1094 | Loss: 0.3667\nStep 700/1094 | Loss: 0.2972\nStep 800/1094 | Loss: 0.2341\nStep 900/1094 | Loss: 0.2641\nStep 1000/1094 | Loss: 0.2388\nAverage Training Loss: 0.3496\nAverage Training Accuracy: 0.8428\nAverage Validation Loss: 0.2906\nValidation Accuracy: 0.8817\n\nEpoch 2/5\nStep 100/1094 | Loss: 0.1546\nStep 200/1094 | Loss: 0.2092\nStep 300/1094 | Loss: 0.1361\nStep 400/1094 | Loss: 0.2721\nStep 500/1094 | Loss: 0.3058\nStep 600/1094 | Loss: 0.1556\nStep 700/1094 | Loss: 0.4430\nStep 800/1094 | Loss: 0.2532\nStep 900/1094 | Loss: 0.1415\nStep 1000/1094 | Loss: 0.1140\nAverage Training Loss: 0.2255\nAverage Training Accuracy: 0.9122\nAverage Validation Loss: 0.2756\nValidation Accuracy: 0.8855\n\nEpoch 3/5\nStep 100/1094 | Loss: 0.1667\nStep 200/1094 | Loss: 0.0158\nStep 300/1094 | Loss: 0.2192\nStep 400/1094 | Loss: 0.1667\nStep 500/1094 | Loss: 0.1051\nStep 600/1094 | Loss: 0.1293\nStep 700/1094 | Loss: 0.0388\nStep 800/1094 | Loss: 0.2242\nStep 900/1094 | Loss: 0.0598\nStep 1000/1094 | Loss: 0.2579\nAverage Training Loss: 0.1460\nAverage Training Accuracy: 0.9489\nAverage Validation Loss: 0.3777\nValidation Accuracy: 0.8796\n\nEpoch 4/5\nStep 100/1094 | Loss: 0.0127\nStep 200/1094 | Loss: 0.0746\nStep 300/1094 | Loss: 0.0061\nStep 400/1094 | Loss: 0.3445\nStep 500/1094 | Loss: 0.0434\nStep 600/1094 | Loss: 0.0965\nStep 700/1094 | Loss: 0.1123\nStep 800/1094 | Loss: 0.2653\nStep 900/1094 | Loss: 0.0421\nStep 1000/1094 | Loss: 0.0080\nAverage Training Loss: 0.0909\nAverage Training Accuracy: 0.9725\nAverage Validation Loss: 0.4410\nValidation Accuracy: 0.8893\nEarly stopping triggered.\n\nTraining with lr=3e-05, batch_size=16\n\nEpoch 1/5\nStep 100/2188 | Loss: 0.4926\nStep 200/2188 | Loss: 0.3705\nStep 300/2188 | Loss: 0.3799\nStep 400/2188 | Loss: 0.1062\nStep 500/2188 | Loss: 0.2358\nStep 600/2188 | Loss: 0.3883\nStep 700/2188 | Loss: 0.4135\nStep 800/2188 | Loss: 0.5079\nStep 900/2188 | Loss: 0.5632\nStep 1000/2188 | Loss: 0.1895\nStep 1100/2188 | Loss: 0.4960\nStep 1200/2188 | Loss: 0.2739\nStep 1300/2188 | Loss: 0.9590\nStep 1400/2188 | Loss: 0.2366\nStep 1500/2188 | Loss: 0.1893\nStep 1600/2188 | Loss: 0.2867\nStep 1700/2188 | Loss: 0.1728\nStep 1800/2188 | Loss: 0.2932\nStep 1900/2188 | Loss: 0.1384\nStep 2000/2188 | Loss: 0.1259\nStep 2100/2188 | Loss: 0.2726\nAverage Training Loss: 0.3456\nAverage Training Accuracy: 0.8507\nAverage Validation Loss: 0.2758\nValidation Accuracy: 0.8821\n\nEpoch 2/5\nStep 100/2188 | Loss: 0.0570\nStep 200/2188 | Loss: 0.2534\nStep 300/2188 | Loss: 0.4481\nStep 400/2188 | Loss: 0.1338\nStep 500/2188 | Loss: 0.0972\nStep 600/2188 | Loss: 0.3286\nStep 700/2188 | Loss: 0.2646\nStep 800/2188 | Loss: 0.2105\nStep 900/2188 | Loss: 0.1976\nStep 1000/2188 | Loss: 0.2066\nStep 1100/2188 | Loss: 0.1451\nStep 1200/2188 | Loss: 0.3952\nStep 1300/2188 | Loss: 0.0991\nStep 1400/2188 | Loss: 0.1385\nStep 1500/2188 | Loss: 0.6184\nStep 1600/2188 | Loss: 0.0071\nStep 1700/2188 | Loss: 0.2079\nStep 1800/2188 | Loss: 0.0062\nStep 1900/2188 | Loss: 0.7250\nStep 2000/2188 | Loss: 0.0258\nStep 2100/2188 | Loss: 0.1646\nAverage Training Loss: 0.2059\nAverage Training Accuracy: 0.9259\nAverage Validation Loss: 0.3228\nValidation Accuracy: 0.8919\n\nEpoch 3/5\nStep 100/2188 | Loss: 0.4438\nStep 200/2188 | Loss: 0.0028\nStep 300/2188 | Loss: 0.0037\nStep 400/2188 | Loss: 0.0038\nStep 500/2188 | Loss: 0.1274\nStep 600/2188 | Loss: 0.0070\nStep 700/2188 | Loss: 0.0891\nStep 800/2188 | Loss: 0.3646\nStep 900/2188 | Loss: 0.0066\nStep 1000/2188 | Loss: 0.2711\nStep 1100/2188 | Loss: 0.0717\nStep 1200/2188 | Loss: 0.0432\nStep 1300/2188 | Loss: 0.0843\nStep 1400/2188 | Loss: 0.0085\nStep 1500/2188 | Loss: 0.0044\nStep 1600/2188 | Loss: 0.2739\nStep 1700/2188 | Loss: 0.0267\nStep 1800/2188 | Loss: 0.0039\nStep 1900/2188 | Loss: 0.4919\nStep 2000/2188 | Loss: 0.0300\nStep 2100/2188 | Loss: 0.0033\nAverage Training Loss: 0.1212\nAverage Training Accuracy: 0.9655\nAverage Validation Loss: 0.4363\nValidation Accuracy: 0.8957\nEarly stopping triggered.\n\nTraining with lr=3e-05, batch_size=32\n\nEpoch 1/5\nStep 100/1094 | Loss: 0.2951\nStep 200/1094 | Loss: 0.4848\nStep 300/1094 | Loss: 0.3288\nStep 400/1094 | Loss: 0.3408\nStep 500/1094 | Loss: 0.2604\nStep 600/1094 | Loss: 0.2640\nStep 700/1094 | Loss: 1.0250\nStep 800/1094 | Loss: 0.4755\nStep 900/1094 | Loss: 0.1972\nStep 1000/1094 | Loss: 0.4319\nAverage Training Loss: 0.3394\nAverage Training Accuracy: 0.8506\nAverage Validation Loss: 0.2830\nValidation Accuracy: 0.8843\n\nEpoch 2/5\nStep 100/1094 | Loss: 0.1303\nStep 200/1094 | Loss: 0.1288\nStep 300/1094 | Loss: 0.0942\nStep 400/1094 | Loss: 0.2423\nStep 500/1094 | Loss: 0.2390\nStep 600/1094 | Loss: 0.2222\nStep 700/1094 | Loss: 0.0459\nStep 800/1094 | Loss: 0.2169\nStep 900/1094 | Loss: 0.1057\nStep 1000/1094 | Loss: 0.2182\nAverage Training Loss: 0.2050\nAverage Training Accuracy: 0.9207\nAverage Validation Loss: 0.2726\nValidation Accuracy: 0.8933\n\nEpoch 3/5\nStep 100/1094 | Loss: 0.1813\nStep 200/1094 | Loss: 0.0929\nStep 300/1094 | Loss: 0.0808\nStep 400/1094 | Loss: 0.1369\nStep 500/1094 | Loss: 0.0082\nStep 600/1094 | Loss: 0.0065\nStep 700/1094 | Loss: 0.0663\nStep 800/1094 | Loss: 0.0263\nStep 900/1094 | Loss: 0.2318\nStep 1000/1094 | Loss: 0.3847\nAverage Training Loss: 0.1128\nAverage Training Accuracy: 0.9632\nAverage Validation Loss: 0.4244\nValidation Accuracy: 0.8889\n\nEpoch 4/5\nStep 100/1094 | Loss: 0.0287\nStep 200/1094 | Loss: 0.0918\nStep 300/1094 | Loss: 0.1849\nStep 400/1094 | Loss: 0.0029\nStep 500/1094 | Loss: 0.0049\nStep 600/1094 | Loss: 0.0043\nStep 700/1094 | Loss: 0.0035\nStep 800/1094 | Loss: 0.0085\nStep 900/1094 | Loss: 0.0074\nStep 1000/1094 | Loss: 0.0029\nAverage Training Loss: 0.0634\nAverage Training Accuracy: 0.9832\nAverage Validation Loss: 0.4994\nValidation Accuracy: 0.8900\nEarly stopping triggered.\n\nTraining with lr=5e-05, batch_size=16\n\nEpoch 1/5\nStep 100/2188 | Loss: 0.4449\nStep 200/2188 | Loss: 0.3663\nStep 300/2188 | Loss: 0.4528\nStep 400/2188 | Loss: 0.0798\nStep 500/2188 | Loss: 0.2004\nStep 600/2188 | Loss: 0.2104\nStep 700/2188 | Loss: 0.4947\nStep 800/2188 | Loss: 0.2805\nStep 900/2188 | Loss: 0.3586\nStep 1000/2188 | Loss: 0.2157\nStep 1100/2188 | Loss: 0.1941\nStep 1200/2188 | Loss: 0.3763\nStep 1300/2188 | Loss: 0.1249\nStep 1400/2188 | Loss: 0.1423\nStep 1500/2188 | Loss: 0.2431\nStep 1600/2188 | Loss: 0.2630\nStep 1700/2188 | Loss: 0.1806\nStep 1800/2188 | Loss: 0.2182\nStep 1900/2188 | Loss: 0.3096\nStep 2000/2188 | Loss: 0.5494\nStep 2100/2188 | Loss: 0.3834\nAverage Training Loss: 0.3491\nAverage Training Accuracy: 0.8506\nAverage Validation Loss: 0.3077\nValidation Accuracy: 0.8824\n\nEpoch 2/5\nStep 100/2188 | Loss: 0.5640\nStep 200/2188 | Loss: 0.0363\nStep 300/2188 | Loss: 0.0930\nStep 400/2188 | Loss: 0.1885\nStep 500/2188 | Loss: 0.2768\nStep 600/2188 | Loss: 0.1667\nStep 700/2188 | Loss: 0.1078\nStep 800/2188 | Loss: 0.2812\nStep 900/2188 | Loss: 0.1459\nStep 1000/2188 | Loss: 0.1619\nStep 1100/2188 | Loss: 0.1551\nStep 1200/2188 | Loss: 0.1993\nStep 1300/2188 | Loss: 0.0499\nStep 1400/2188 | Loss: 0.4113\nStep 1500/2188 | Loss: 0.0635\nStep 1600/2188 | Loss: 0.0348\nStep 1700/2188 | Loss: 0.5280\nStep 1800/2188 | Loss: 0.0479\nStep 1900/2188 | Loss: 0.1642\nStep 2000/2188 | Loss: 0.0101\nStep 2100/2188 | Loss: 0.3769\nAverage Training Loss: 0.1975\nAverage Training Accuracy: 0.9316\nAverage Validation Loss: 0.3151\nValidation Accuracy: 0.8892\n\nEpoch 3/5\nStep 100/2188 | Loss: 0.0052\nStep 200/2188 | Loss: 0.2509\nStep 300/2188 | Loss: 0.3315\nStep 400/2188 | Loss: 0.2709\nStep 500/2188 | Loss: 0.0105\nStep 600/2188 | Loss: 0.0026\nStep 700/2188 | Loss: 0.0362\nStep 800/2188 | Loss: 0.0114\nStep 900/2188 | Loss: 0.0051\nStep 1000/2188 | Loss: 0.0035\nStep 1100/2188 | Loss: 0.1235\nStep 1200/2188 | Loss: 0.0021\nStep 1300/2188 | Loss: 0.0033\nStep 1400/2188 | Loss: 0.0020\nStep 1500/2188 | Loss: 0.3150\nStep 1600/2188 | Loss: 0.0786\nStep 1700/2188 | Loss: 0.0061\nStep 1800/2188 | Loss: 0.3247\nStep 1900/2188 | Loss: 0.0029\nStep 2000/2188 | Loss: 0.0030\nStep 2100/2188 | Loss: 0.0066\nAverage Training Loss: 0.1003\nAverage Training Accuracy: 0.9731\nAverage Validation Loss: 0.4952\nValidation Accuracy: 0.8811\nEarly stopping triggered.\n\nTraining with lr=5e-05, batch_size=32\n\nEpoch 1/5\nStep 100/1094 | Loss: 0.3546\nStep 200/1094 | Loss: 0.2854\nStep 300/1094 | Loss: 0.4467\nStep 400/1094 | Loss: 0.2759\nStep 500/1094 | Loss: 0.3650\nStep 600/1094 | Loss: 0.2634\nStep 700/1094 | Loss: 0.4572\nStep 800/1094 | Loss: 0.3619\nStep 900/1094 | Loss: 0.1945\nStep 1000/1094 | Loss: 0.2393\nAverage Training Loss: 0.3413\nAverage Training Accuracy: 0.8493\nAverage Validation Loss: 0.2677\nValidation Accuracy: 0.8851\n\nEpoch 2/5\nStep 100/1094 | Loss: 0.2798\nStep 200/1094 | Loss: 0.0298\nStep 300/1094 | Loss: 0.2297\nStep 400/1094 | Loss: 0.0717\nStep 500/1094 | Loss: 0.1435\nStep 600/1094 | Loss: 0.2815\nStep 700/1094 | Loss: 0.0542\nStep 800/1094 | Loss: 0.0412\nStep 900/1094 | Loss: 0.2611\nStep 1000/1094 | Loss: 0.1246\nAverage Training Loss: 0.1816\nAverage Training Accuracy: 0.9323\nAverage Validation Loss: 0.3017\nValidation Accuracy: 0.8899\n\nEpoch 3/5\nStep 100/1094 | Loss: 0.3764\nStep 200/1094 | Loss: 0.1573\nStep 300/1094 | Loss: 0.1152\nStep 400/1094 | Loss: 0.1682\nStep 500/1094 | Loss: 0.0174\nStep 600/1094 | Loss: 0.0448\nStep 700/1094 | Loss: 0.1309\nStep 800/1094 | Loss: 0.0510\nStep 900/1094 | Loss: 0.3072\nStep 1000/1094 | Loss: 0.0029\nAverage Training Loss: 0.0846\nAverage Training Accuracy: 0.9750\nAverage Validation Loss: 0.4404\nValidation Accuracy: 0.8877\nEarly stopping triggered.\n\nBest Hyperparameters: {'lr': 3e-05, 'batch_size': 16}\nBest Validation Accuracy: 0.8957\nTraining complete. Best model saved as 'best_model.pt'. Loss and accuracy curves saved.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\nimport pickle\nimport time\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load model and tokenizer\nmodel = DistilBertForSequenceClassification.from_pretrained('sentiment_model')\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.to(device)\nmodel.eval()\ntokenizer = DistilBertTokenizer.from_pretrained('sentiment_model')\n\n# Load label encoder\nwith open('label_encoder.pkl', 'rb') as f:\n    label_encoder = pickle.load(f)\n\n# Load test data\ntest_df = pd.read_csv('test_data.csv')\n\n# Tokenize test data\ndef tokenize_data(texts, labels, tokenizer, max_length=128):\n    input_ids = []\n    attention_masks = []\n    \n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels.values, dtype=torch.long)\n    \n    return input_ids, attention_masks, labels\n\ntest_inputs, test_masks, test_labels = tokenize_data(test_df['processed_text'], test_df['sentiment_encoded'], tokenizer)\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\nbatch_size = 16\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n# Evaluate on test set\ntest_predictions, test_true_labels = [], []\ninference_times = []\n\nfor batch in test_dataloader:\n    b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]\n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model(b_input_ids, attention_mask=b_input_mask)\n    inference_times.append(time.time() - start_time)\n    logits = outputs.logits\n    test_predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n    test_true_labels.extend(b_labels.cpu().numpy())\n\n# Calculate metrics\ntest_accuracy = accuracy_score(test_true_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_true_labels, test_predictions, average='weighted')\n\navg_inference_time = np.mean(inference_times)\n\nprint(\"\\nTest Set Evaluation:\")\nprint(f\"Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")\nprint(f\"Average Inference Time: {avg_inference_time:.4f} seconds\")\n\n# Confusion Matrix\ncm = confusion_matrix(test_true_labels, test_predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.savefig('confusion_matrix.png')\nplt.close()\n\n# Save evaluation metrics\nmetrics = {\n    'accuracy': test_accuracy,\n    'precision': precision,\n    'recall': recall,\n    'f1': f1,\n    'avg_inference_time': avg_inference_time\n}\nwith open('evaluation_metrics.pkl', 'wb') as f:\n    pickle.dump(metrics, f)\n\nprint(\"Evaluation complete. Confusion matrix saved as 'confusion_matrix.png'. Metrics saved as 'evaluation_metrics.pkl'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T16:40:28.019538Z","iopub.execute_input":"2025-04-27T16:40:28.019839Z","iopub.status.idle":"2025-04-27T16:41:11.848176Z","shell.execute_reply.started":"2025-04-27T16:40:28.019818Z","shell.execute_reply":"2025-04-27T16:41:11.847421Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1153040831.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"\nTest Set Evaluation:\nAccuracy: 0.8983\nPrecision: 0.8983\nRecall: 0.8983\nF1-Score: 0.8983\nAverage Inference Time: 0.0042 seconds\nEvaluation complete. Confusion matrix saved as 'confusion_matrix.png'. Metrics saved as 'evaluation_metrics.pkl'\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def predict_sentiment(text, model, tokenizer):\n    # Preprocess the text\n    processed_text = preprocess_text(text)\n    \n    # Tokenize\n    encoded_dict = tokenizer.encode_plus(\n        processed_text,\n        add_special_tokens=True,\n        max_length=128,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    # Move to device\n    input_ids = encoded_dict['input_ids'].to(device)\n    attention_mask = encoded_dict['attention_mask'].to(device)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Get prediction\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n    \n    # Get prediction class\n    prediction = torch.argmax(logits, dim=1).item()\n    \n    # Map prediction to sentiment using label encoder\n    sentiment = label_encoder.inverse_transform([prediction])[0]\n    \n    return sentiment\n\n# Example usage\nprint(\"\\nSample Predictions:\")\nsample_texts = [\n    \"I absolutely love this product! It's amazing!\",\n    \"This is the worst experience I've ever had.\",\n    \"The movie was fantastic, with great acting and a compelling story.\",\n    \"I was really disappointed with the service at this restaurant.\"\n]\n\nmodel = DistilBertForSequenceClassification.from_pretrained('sentiment_model')\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.to(device)\ntokenizer = DistilBertTokenizer.from_pretrained('sentiment_model')\nfor text in sample_texts:\n    sentiment = predict_sentiment(text, model, tokenizer)\n    print(f\"Text: {text}\")\n    print(f\"Sentiment: {sentiment}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T16:41:30.647069Z","iopub.execute_input":"2025-04-27T16:41:30.647566Z","iopub.status.idle":"2025-04-27T16:41:31.414319Z","shell.execute_reply.started":"2025-04-27T16:41:30.647544Z","shell.execute_reply":"2025-04-27T16:41:31.413733Z"}},"outputs":[{"name":"stdout","text":"\nSample Predictions:\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3517145775.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Text: I absolutely love this product! It's amazing!\nSentiment: positive\n\nText: This is the worst experience I've ever had.\nSentiment: negative\n\nText: The movie was fantastic, with great acting and a compelling story.\nSentiment: positive\n\nText: I was really disappointed with the service at this restaurant.\nSentiment: negative\n\n","output_type":"stream"}],"execution_count":57}]}